{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebdfc55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import *\n",
    "import time\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "\n",
    "def forward_pass(W1, W2, W3, b1, b2, b3, x):\n",
    "    \"\"\"\n",
    "    forward-pass for an fully connected neural network with 2 hidden layers of M neurons\n",
    "    Inputs:\n",
    "        W1 : (M, 784) weights of first (hidden) layer\n",
    "        W2 : (M, M) weights of second (hidden) layer\n",
    "        W3 : (10, M) weights of third (output) layer\n",
    "        b1 : (M, 1) biases of first (hidden) layer\n",
    "        b2 : (M, 1) biases of second (hidden) layer\n",
    "        b3 : (10, 1) biases of third (output) layer\n",
    "        x : (N, 784) training inputs\n",
    "    Outputs:\n",
    "        Fhat : (N, 10) output of the neural network at training inputs\n",
    "    \"\"\"\n",
    "    H1 = np.maximum(0, np.dot(x, W1.T) + b1.T) # layer 1 neurons with ReLU activation, shape (N, M)\n",
    "    \n",
    "    #print(np.min(H1),np.max(H1))\n",
    "    H2 = np.maximum(0, np.dot(H1, W2.T) + b2.T) # layer 2 neurons with ReLU activation, shape (N, M)\n",
    "    \n",
    "    #print(np.min(H2),np.max(H2))\n",
    "    Fhat = np.dot(H2, W3.T) + b3.T # layer 3 (output) neurons with linear activation, shape (N, 10)\n",
    "\n",
    "    N = np.shape(Fhat)[0]\n",
    "    \n",
    "    Fhat_max = np.amax(Fhat,axis=1).reshape(N,1)\n",
    "    \n",
    "    diff = Fhat-Fhat_max\n",
    "       \n",
    "    exp_diff = np.exp(diff)\n",
    "    \n",
    "    sum_exp_diff = np.sum(exp_diff,axis=1).reshape(N,1)\n",
    "    \n",
    "    log_sum_exp_diff = np.log(sum_exp_diff)\n",
    "    \n",
    "    return diff - log_sum_exp_diff\n",
    "\n",
    "\n",
    "def negative_log_likelihood(W1, W2, W3, b1, b2, b3, x, y):\n",
    "    \"\"\"\n",
    "    computes the negative log likelihood of the model `forward_pass`\n",
    "    Inputs:\n",
    "        W1, W2, W3, b1, b2, b3, x : same as `forward_pass`\n",
    "        y : (N, 10) training responses\n",
    "    Outputs:\n",
    "        nll : negative log likelihood\n",
    "    \"\"\"\n",
    "    Fhat = forward_pass(W1, W2, W3, b1, b2, b3, x)\n",
    "\n",
    "    nll = -(Fhat*y)\n",
    "    \n",
    "    return np.sum(nll)\n",
    "    \n",
    "\n",
    "nll_gradients = value_and_grad(negative_log_likelihood, argnum=[0,1,2,3,4,5])\n",
    "\"\"\"\n",
    "    returns the output of `negative_log_likelihood` as well as the gradient of the \n",
    "    output with respect to all weights and biases\n",
    "    Inputs:\n",
    "        same as negative_log_likelihood (W1, W2, W3, b1, b2, b3, x, y)\n",
    "    Outputs: (nll, (W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad))\n",
    "        nll : output of `negative_log_likelihood`\n",
    "        W1_grad : (M, 784) gradient of the nll with respect to the weights of first (hidden) layer\n",
    "        W2_grad : (M, M) gradient of the nll with respect to the weights of second (hidden) layer\n",
    "        W3_grad : (10, M) gradient of the nll with respect to the weights of third (output) layer\n",
    "        b1_grad : (M, 1) gradient of the nll with respect to the biases of first (hidden) layer\n",
    "        b2_grad : (M, 1) gradient of the nll with respect to the biases of second (hidden) layer\n",
    "        b3_grad : (10, 1) gradient of the nll with respect to the biases of third (output) layer\n",
    "     \"\"\"\n",
    "\n",
    "def update_parameters(w, grad_w, learning_rate=1.):\n",
    "    \"\"\"\n",
    "    perform gradient descent update to minimize an objective\n",
    "    Inputs:\n",
    "        w : vector of parameters\n",
    "        grad_w : gradient of the loss with respect to the parameters\n",
    "        learning_rate : learning rate of the optimizer\n",
    "    \"\"\"\n",
    "    return w - learning_rate * grad_w\n",
    "\n",
    "# load the MNIST_small dataset\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('mnist_small')\n",
    "\n",
    "#scale data to 4-bit black and white\n",
    "for i in range (x_train.shape[0]):\n",
    "    l = x_train[i].min()\n",
    "    h = x_train[i].max()\n",
    "    x_train[i] = (((x_train[i]-l)/(h-l))*15).astype(np.uint8)\n",
    "    \n",
    "for i in range (x_test.shape[0]):\n",
    "    l = x_test[i].min()\n",
    "    h = x_test[i].max()\n",
    "    x_test[i] = (((x_test[i]-l)/(h-l))*15).astype(np.uint8)\n",
    "    \n",
    "for i in range (x_valid.shape[0]):\n",
    "    l = x_valid[i].min()\n",
    "    h = x_valid[i].max()\n",
    "    x_valid[i] = (((x_valid[i]-l)/(h-l))*15).astype(np.uint8)\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "scale = 0.1\n",
    "M = 10 # neurons per hidden layer\n",
    "W1 = scale *np.random.randn(M, 784) # weights of first (hidden) layer\n",
    "W2 = scale *np.random.randn(M, M) # weights of second (hidden) layer\n",
    "W3 = scale *np.random.randn(10, M) # weights of third (output) layer\n",
    "b1 = np.zeros((M, 1)) # biases of first (hidden) layer\n",
    "b2 = np.zeros((M, 1)) # biases of second (hidden) layer\n",
    "b3 = np.zeros((10, 1)) # biases of third (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5809274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   1, loss = 23176.645392\n",
      "Iter   1, Validation Loss = 2318.898576\n",
      "Iter 200, loss = 5121.993508\n",
      "Iter 200, Validation Loss = 553.941933\n",
      "Iter 400, loss = 3532.761319\n",
      "Iter 400, Validation Loss = 441.581490\n",
      "Iter 600, loss = 2920.872742\n",
      "Iter 600, Validation Loss = 400.008008\n",
      "Iter 800, loss = 3004.763573\n",
      "Iter 800, Validation Loss = 409.587109\n",
      "Iter 1000, loss = 2480.932017\n",
      "Iter 1000, Validation Loss = 387.414678\n",
      "Iter 1200, loss = 2278.807493\n",
      "Iter 1200, Validation Loss = 371.790168\n",
      "Iter 1400, loss = 1928.972958\n",
      "Iter 1400, Validation Loss = 357.195005\n",
      "Iter 1600, loss = 1881.171284\n",
      "Iter 1600, Validation Loss = 361.586945\n",
      "Iter 1800, loss = 1867.239615\n",
      "Iter 1800, Validation Loss = 350.444879\n",
      "Iter 2000, loss = 1561.761158\n",
      "Iter 2000, Validation Loss = 353.347935\n",
      "Iter 2200, loss = 1705.082528\n",
      "Iter 2200, Validation Loss = 384.947216\n",
      "Iter 2400, loss = 1486.688218\n",
      "Iter 2400, Validation Loss = 373.101159\n",
      "Iter 2600, loss = 2206.290896\n",
      "Iter 2600, Validation Loss = 400.798510\n",
      "Iter 2800, loss = 1336.700397\n",
      "Iter 2800, Validation Loss = 366.904457\n",
      "Iter 3000, loss = 1282.854829\n",
      "Iter 3000, Validation Loss = 367.945724\n"
     ]
    }
   ],
   "source": [
    "#set up training paramaters\n",
    "learning_rate = 0.0002\n",
    "size = x_train.shape[0]\n",
    "batch_size = 250\n",
    "\n",
    "n = 3000\n",
    "iteration_vec = np.zeros((round(n/10),1))\n",
    "log_loss_vec = np.zeros((round(n/10),1))\n",
    "v_log_loss_vec = np.zeros((round(n/10),1))\n",
    "\n",
    "for i in range(n):\n",
    "    #select minibatch\n",
    "    randomize = np.arange(size)\n",
    "    np.random.shuffle(randomize)\n",
    "    x_train = x_train[randomize]\n",
    "    y_train = y_train[randomize]\n",
    "    \n",
    "    # compute the gradient\n",
    "    (nll, (W1_grad, W2_grad, W3_grad, b1_grad, b2_grad, b3_grad)) = nll_gradients(W1, W2, W3, b1, b2, b3, x_train[:batch_size], y_train[:batch_size])\n",
    "    # update the parameters\n",
    "    W1 = update_parameters(W1, W1_grad, learning_rate)\n",
    "    W2 = update_parameters(W2, W2_grad, learning_rate)\n",
    "    W3 = update_parameters(W3, W3_grad, learning_rate)\n",
    "    b1 = update_parameters(b1, b1_grad, learning_rate)\n",
    "    b2 = update_parameters(b2, b2_grad, learning_rate)\n",
    "    b3 = update_parameters(b3, b3_grad, learning_rate)\n",
    "    \n",
    "    #record performance\n",
    "    if (i+1) % 10 == 0:\n",
    "        log_loss_vec[round(i/10)-1] = negative_log_likelihood(W1, W2, W3, b1, b2, b3, x_train[:batch_size], y_train[:batch_size])\n",
    "        v_log_loss_vec[round(i/10)-1] = negative_log_likelihood(W1, W2, W3, b1, b2, b3, x_valid,y_valid)\n",
    "        iteration_vec[round(i/10)-1] = i\n",
    "        \n",
    "    # print loss if nessessary\n",
    "    if i==0 or (i+1) % 200 == 0:\n",
    "        print(\"Iter %3d, loss = %.6f\" % (i+1, negative_log_likelihood(W1, W2, W3, b1, b2, b3, x_train, y_train)))\n",
    "        val = negative_log_likelihood(W1, W2, W3, b1, b2, b3, x_valid,y_valid)  \n",
    "        print(\"Iter %3d, Validation Loss = %.6f\" % (i+1, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "759641df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on testing data = 91.4 %\n"
     ]
    }
   ],
   "source": [
    "#find the accuracy of the trained model\n",
    "\n",
    "preds = forward_pass(W1, W2, W3, b1, b2, b3, x_test)\n",
    "probs = np.amax(np.exp(preds),axis = 1)\n",
    "pred_idx = np.argmax(preds,axis = 1)\n",
    "targets = np.argmax(y_test,axis = 1)\n",
    "\n",
    "#calculate number of correct predictions on test set\n",
    "correct = np.sum(pred_idx==targets)\n",
    "\n",
    "#number of test points\n",
    "total = len(targets)\n",
    "\n",
    "print('accuracy on testing data =',correct*100/total,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68fd56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on testing data = 92.0 %\n"
     ]
    }
   ],
   "source": [
    "#cast params into ints\n",
    "W1_new = (W1*1e3).astype(np.int16)\n",
    "W2_new = (W2*1e3).astype(np.int16)\n",
    "W3_new = (W3*1e3).astype(np.int16)\n",
    "\n",
    "b1_new = (b1*1e3).astype(np.int16)\n",
    "b2_new = (b2*1e3).astype(np.int16)\n",
    "b3_new = (b3*1e3).astype(np.int16)\n",
    "\n",
    "preds = forward_pass(W1_new, W2_new, W3_new, b1_new, b2_new, b3_new, x_test)\n",
    "probs = np.amax(np.exp(preds),axis = 1)\n",
    "pred_idx = np.argmax(preds,axis = 1)\n",
    "targets = np.argmax(y_test,axis = 1)\n",
    "\n",
    "#calculate number of correct predictions on test set\n",
    "correct = np.sum(pred_idx==targets)\n",
    "\n",
    "#number of test points\n",
    "total = len(targets)\n",
    "\n",
    "print('accuracy on testing data =',correct*100/total,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dddcac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.13617858e+10, -4.43701774e+10, -1.32617229e+10,\n",
       "         0.00000000e+00, -2.43594086e+10, -1.62651976e+10,\n",
       "        -4.47055490e+10, -2.89100882e+10, -2.08626348e+10,\n",
       "        -2.06895244e+10]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass(W1_new, W2_new, W3_new, b1_new, b2_new, b3_new, 15*np.ones([784,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "648f2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 15*np.ones([784,])\n",
    "\n",
    "H1 = np.maximum(0, np.dot(x, W1_new.T) + b1_new.T) # layer 1 neurons with ReLU activation, shape (N, M)\n",
    "\n",
    "#print(np.min(H1),np.max(H1))\n",
    "H2 = np.maximum(0, np.dot(H1, W2_new.T) + b2_new.T) # layer 2 neurons with ReLU activation, shape (N, M)\n",
    "\n",
    "#print(np.min(H2),np.max(H2))\n",
    "Fhat = np.dot(H2, W3_new.T) + b3_new.T # layer 3 (output) neurons with linear activation, shape (N, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f5b29822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13833630e+07, -6.15938500e+07, -1.07835520e+07,\n",
       "         4.48475600e+06,  3.66800290e+07,  2.94925520e+07,\n",
       "         1.78744580e+07, -6.03236250e+07,  3.91116720e+07,\n",
       "        -1.61259666e+08]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(H1, W2_new.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5f450dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21383273.,        0.,        0.,  4484783., 36680023., 29492550.,\n",
       "        17874525.,        0., 39111637.,        0.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d358cfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.69203492e+09, -2.03163567e+10,  1.07920978e+10,\n",
       "         2.40538207e+10, -3.05587941e+08,  7.78862310e+09,\n",
       "        -2.06517284e+10, -4.85626756e+09,  3.19118584e+09,\n",
       "         3.36429625e+09]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fhat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44627a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a new directory to store the output files\n",
    "os.makedirs('output_folder', exist_ok=True)\n",
    "\n",
    "#W1\n",
    "for k in range(10):\n",
    "    # Open a new file for writing, with a filename based on the value of k\n",
    "    with open(f'output_folder/W1_{k}.mem', 'w') as f:\n",
    "        # Loop over i values from 0 to 783\n",
    "        for i in range(784): \n",
    "            # Write the value of w[i][k] to the file, followed by a newline character\n",
    "            f.write('{:0>4x}\\n'.format(W1_new[k][i] & 0xffff))\n",
    "            \n",
    "#W2\n",
    "for k in range(10):\n",
    "    # Open a new file for writing, with a filename based on the value of k\n",
    "    with open(f'output_folder/W2_{k}.mem', 'w') as f:\n",
    "        # Loop over i values from 0 to 783\n",
    "        for i in range(10): \n",
    "            # Write the value of w[i][k] to the file, followed by a newline character\n",
    "            f.write('{:0>4x}\\n'.format(W2_new[k][i] & 0xffff))\n",
    "                   \n",
    "#W3\n",
    "for k in range(10):\n",
    "    # Open a new file for writing, with a filename based on the value of k\n",
    "    with open(f'output_folder/W3_{k}.mem', 'w') as f:\n",
    "        # Loop over i values from 0 to 783\n",
    "        for i in range(10): \n",
    "            # Write the value of w[i][k] to the file, followed by a newline character\n",
    "            f.write('{:0>4x}\\n'.format(W3_new[k][i] & 0xffff))\n",
    "            \n",
    "#b1            \n",
    "with open(f'output_folder/B1_0.mem', 'w') as f:\n",
    "    # Loop over i values from 0 to 783\n",
    "    for i in range(10): \n",
    "        # Write the value of w[i][k] to the file, followed by a newline character\n",
    "        f.write('{:0>4x}\\n'.format(b1_new[i][0] & 0xffff))\n",
    "\n",
    "#b2\n",
    "with open(f'output_folder/B2_0.mem', 'w') as f:\n",
    "    # Loop over i values from 0 to 783\n",
    "    for i in range(10): \n",
    "        # Write the value of w[i][k] to the file, followed by a newline character\n",
    "        f.write('{:0>4x}\\n'.format(b2_new[i][0] & 0xffff))\n",
    "\n",
    "#b3\n",
    "with open(f'output_folder/B3_0.mem', 'w') as f:\n",
    "    # Loop over i values from 0 to 783\n",
    "    for i in range(10): \n",
    "        # Write the value of w[i][k] to the file, followed by a newline character\n",
    "        f.write('{:0>4x}\\n'.format(b3_new[i][0] & 0xffff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65bd55a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bf67e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 121, -194,  -80,    4,  -59,   86, -208,   36,   42,    4,  110,\n",
       "       -122,  110,  -70,   72,  -32,   81,   78, -146,  -15,   -9,  -23,\n",
       "        -75,  185,   20,  155,  -56, -106,   13,  -56,  239,   24,  115,\n",
       "        -22,  -33,   -8,   11,   77,  -15,  -66, -101,   -9,   41,    3,\n",
       "        -29,  -67,   14,   56,   -3,   10,  -15,   91,  -43,   18,   39,\n",
       "         72,  149,   67,   59, -147,   60,  229,  -84, -108,  -49,   79,\n",
       "         32,  -75,  -50,   28,   -6,  178,   82,   23,   40,  -61, -109,\n",
       "          5,   43, -139,  -52,  -36,   26,  -26,   44,    9,  106, -171,\n",
       "        165,  142,    1,   48,   20, -110,  -33,   21,  -68,  -16,  -74,\n",
       "       -140, -145,  -93, -102,  143,   -9, -128,   35,  -16, -304,   47,\n",
       "       -155,   57,  -96, -145,   49, -148,  -43,   15,   51,   32,  -57,\n",
       "        -44,  136, -128,   76, -172,    7,   21, -155,   17,   14,  -33,\n",
       "        -18,  112,   17, -116,  125,  -44,  137,   60,  -89,  -16,  -28,\n",
       "        -90,   56,    5,  184,   19, -181,   16,   18,   -9,  245,  149,\n",
       "        104, -121,   66,   74,  117,   39,  -14,  -54,  -23,   18,  158,\n",
       "       -102,  -20,   23,   43,   91,  -94,  -43,  140,   83,  -50,  -30,\n",
       "          5,  150,   60,   -5,   21,   97,  -13,   80,  201,   39,  125,\n",
       "         74,  117,  -83,  137,   41, -224,  -23,   41,  -22,   58, -100,\n",
       "         22,   27,   12,   -3,  -27,   70,  -64,    1,   92,   45,   79,\n",
       "        164,  190,  152,    8,  101,  117,    7,  156, -154,  105,  -75,\n",
       "       -138,  -65,  -80,   60,   56,  148,   73,  -99, -108,  121,  125,\n",
       "         42,  153,  -81, -118,   -6,   28,  -31,  149,  188,  197,  132,\n",
       "        -22,  142,   85,   -6,   64,  -82,  104,  -65,  129,   19,  256,\n",
       "        -45,   -4,   60, -166,   12,   82,   80,  -31,  -66,   58,  139,\n",
       "          7,  -41,   -7,   46,  -65,  -38,  106,  254,   97,  115,   44,\n",
       "         -8,  -20,  -21,  -45,  -14,  -66,   26,   11,  -79,  -21, -105,\n",
       "       -161,   26,  138,  174,   89,  -44,  -30,   24,  -34,   25,  108,\n",
       "         88,   84,   89,  190,   20,   80,   10,   12, -178,  131,   22,\n",
       "         57,   20,  -10,    8,  -18,  -45,  215,   -5,    8,  186,  -50,\n",
       "        -34,   10, -167, -105,  -47,    2,  200,   77,   95,  142,  126,\n",
       "        -31,   15, -239,    8,  -35,  179,  -68,  114, -246,  106,    8,\n",
       "         54,   57,   12,   96,   20, -101,  218,  133, -140,  122,    6,\n",
       "         25,   87,  102,  106, -100,   72,  -19,  -86, -152,   54,  -96,\n",
       "        -44,  -82,  -11,  -42,  -13,   79,  204,  -64,   78,  -12,  101,\n",
       "        127, -106, -110,   36,   -2,   21,  138,  -53,   34,   67,  -12,\n",
       "        166,  -15,   91, -176,   95,  -84,  -40,  -98,  -94, -132,   38,\n",
       "       -114,  -91,  -88,   83,  -22,  -69,   37,  -56,  120,   92, -132,\n",
       "         62,  -12,   19,  170,  151,   61, -201,  -36, -180, -112,   11,\n",
       "        -13,   -8,  -21,  115,  223,  -42,  106,  176,   61, -135,  -82,\n",
       "         -2,  -89,   43,   28, -157,    0,   -6,   -7,   36,  106,   21,\n",
       "         88,   14,   13,  122,   14,  247,   98,  112,  129,    8,  135,\n",
       "         41,  -20,   -4,   41,  -59,  134,   74,   26,  -62,  219,  -54,\n",
       "       -177,   56,  -77,   86,  -36,   50,   38, -127,  -19, -110,  116,\n",
       "        125,  -63,   48,    0,   -5,   76,   84,  -55,  -65,   12,  -43,\n",
       "         83,   52, -130,   39,   76, -119, -136,  -71,  180,  141,  104,\n",
       "        172, -100, -101,   85, -138, -145,   -5,  -11, -127,   65,   99,\n",
       "         51,   19, -134,  -33,  -75, -115,  -81, -188,  -63, -251,  -73,\n",
       "          5,  -43, -109,  -54,  -25,  -87,  -52, -195,   92, -123,  110,\n",
       "        -21,  -93,   17,   58,   82,   83,   54,  -11,   96,  -70,  -43,\n",
       "          5,  -13, -268,  -62,  -17, -133, -122,  -71,    5,  -15, -108,\n",
       "        -13,  -67,   -5,  -17,   11,  -71,  -23,  -28,  -37,  282,  -84,\n",
       "       -114,  -77,  -57,   -8,   91,  135,  -90, -255,   70,   13,  -18,\n",
       "       -108,  106,  -41,  -20,  -37,   13,  -38,   61,  -22,   69,  -58,\n",
       "        -58,   37,  121,   49,    5,   72, -154,  160, -155,   57, -128,\n",
       "         61, -110,   82,  -24, -160,   53, -113, -122, -126,  151, -101,\n",
       "         25, -117,    2,  -27, -256,  267, -121,   93,   65,   64,  -23,\n",
       "         18,  122,  -31,   26,   91,  -79,  -20,   49,  -65, -103, -138,\n",
       "       -168,   -2,   -6, -167,  -62,  -43,   99,   41,  -14,   19,  -83,\n",
       "         42, -120,  -57,   25,  216,   74,   27,  -17, -132,   92,   20,\n",
       "         37,  -72,   90,  -85,  -37,  106,  126,   25,   74,   93,   89,\n",
       "        -79,  -38,  -15,  -56, -165,    9,    3,    8,    5,  -23,   -1,\n",
       "        133,  140, -150,   13,  -92,  -81,  105,   18, -140,  241,  123,\n",
       "         66, -103,   84, -109,  -30,   74,   34,   36,   37,    6,  190,\n",
       "        100,  194,  -83,    8,  -47,   80,  -21,  -25, -132,   30,  120,\n",
       "        138, -250,  -95,  281,   93,  106,  105,   74,  152,  159,  236,\n",
       "        -85,  197,  125,  117,  370,  113,  219,   25, -120,  -14,  117,\n",
       "        -18,   54,   18,    4,  -41,   13, -201,  -40, -166,   46,   70,\n",
       "        -29,  107,   78,  117,   57,  126,  155,   24,   36,  207, -175,\n",
       "       -203, -134,   45,   16,   49,    7,  153,  157,  115, -115,   36,\n",
       "        -86,  -50, -102,    1,   54,    8, -118, -146,  -68,  -74,   82,\n",
       "        -29,  -13,   -8,  115,  167,   43,  -21,  141,   44,  -42,  101,\n",
       "         20, -143,   62], dtype=int16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1_new[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba259b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
